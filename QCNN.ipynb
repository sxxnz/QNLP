{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Natural Language Processing Prototype\n",
    "### Shengxin Zhuang (21463227)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    labels, sentences = [], []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            t = line[0]\n",
    "            # labels.append([t, 1-t])\n",
    "            labels.append(int(t))\n",
    "            sentences.append(line[1:].strip())\n",
    "    return labels, sentences\n",
    "\n",
    "train_l, train_raw_data = read_data('../datasets/mc_train_data.txt')\n",
    "dev_l, dev_raw_data = read_data('../datasets/mc_dev_data.txt')\n",
    "test_l, test_raw_data = read_data('../datasets/mc_test_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array([train_l]).astype('float64').T\n",
    "dev_labels   = np.array([dev_l]).astype('float64').T\n",
    "test_labels   = np.array([test_l]).astype('float64').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* A raw text pre-processing method.\n",
    "* This was adapted from a project from Lorenz et al. (2021) QNLP paper.\n",
    "* From:\n",
    "* https://github.com/CQCL/qnlp_lorenz_etal_2021_resources/blob/main/code/mc_task.ipynb\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "vocab = dict()          # dictionary to be filled with the vocabulary in the form { word : POStag }\n",
    "data  = dict()           # dictionary to be filled with all the data (train, dev and test subsets); entries of the \n",
    "                        # form { sentence : label } with label encoding '1' as [1.0, 0.0] and '0' as [0.0, 1.0]\n",
    "training_data = []      # list of sentences in the train dataset as strings \"word1 word2 ...\"\n",
    "dev_data      = []           # list of sentences in the dev dataset as strings \"word1 word2 ...\"\n",
    "testing_data  = [] \n",
    "\n",
    "train_token = []\n",
    "dev_token   = []\n",
    "test_token  = []\n",
    "\n",
    "for sent in train_raw_data:\n",
    "    words = sent.split() \n",
    "    sent_untagged = ''\n",
    "    for word in words:\n",
    "        word_untagged, tag = word.split('_')\n",
    "        vocab[word_untagged] = tag\n",
    "        sent_untagged += word_untagged + ' '\n",
    "    sentence = sent_untagged[:-1]\n",
    "    train_token.append(sentence.split())\n",
    "    training_data.append(sentence)\n",
    "    label = np.array([1.0, 0.0]) if sent[0] == '1' else np.array([0.0, 1.0])\n",
    "    data[sentence] = label\n",
    "\n",
    "# Go through the dev data\n",
    "for sent in dev_raw_data:\n",
    "    words = sent.split() \n",
    "    sent_untagged = ''\n",
    "    for word in words:\n",
    "        word_untagged, tag = word.split('_')\n",
    "        vocab[word_untagged] = tag\n",
    "        sent_untagged += word_untagged + ' '\n",
    "    sentence = sent_untagged[:-1]\n",
    "    dev_token.append(sentence.split())\n",
    "    dev_data.append(sentence)\n",
    "    label = np.array([1.0, 0.0]) if sent[0] == '1' else np.array([0.0, 1.0])\n",
    "    data[sentence] = label\n",
    "\n",
    "for sent in test_raw_data:\n",
    "    words = sent.split() \n",
    "    sent_untagged = ''\n",
    "    for word in words:\n",
    "        word_untagged, tag = word.split('_')\n",
    "        vocab[word_untagged] = tag\n",
    "        sent_untagged += word_untagged + ' '\n",
    "    sentence = sent_untagged[:-1]\n",
    "    test_token.append(sentence.split())\n",
    "    testing_data.append(sentence)\n",
    "    label = np.array([1.0, 0.0]) if sent[0] == '1' else np.array([0.0, 1.0])\n",
    "    data[sentence] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = []\n",
    "test_vec = []\n",
    "\n",
    "for sent in train_token:\n",
    "    vec = []\n",
    "    for word in sent:\n",
    "        vec.append(wv[word])\n",
    "    train_vec.append(vec)\n",
    "\n",
    "for sent in test_token:\n",
    "    vec = []\n",
    "    for word in sent:\n",
    "        vec.append(wv[word])\n",
    "    test_vec.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent_vec = []\n",
    "test_sent_vec = []\n",
    "\n",
    "for sent in train_vec:\n",
    "    all_vec = np.stack(sent,axis=0)\n",
    "    sent_v = all_vec.mean(axis=0)\n",
    "    train_sent_vec.append(sent_v)\n",
    "\n",
    "for sent in test_vec:\n",
    "    all_vec = np.stack(sent,axis=0)\n",
    "    sent_v = all_vec.mean(axis=0)\n",
    "    test_sent_vec.append(sent_v)\n",
    "\n",
    "train_sent_vec = np.stack(train_sent_vec)\n",
    "test_sent_vec = np.stack(test_sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* Truncated SVD \n",
    "* This was adapted from CITS4012 lab code\n",
    "* From:\n",
    "* https://weiliu2k.github.io/CITS4012/embeddings/svd.html\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "def reduce_to_k_dim(M, k=2):\n",
    "    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
    "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
    "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "    \n",
    "        Params:\n",
    "            M (numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): co-occurence matrix of word counts\n",
    "            k (int): embedding size of each word after dimension reduction\n",
    "        Return:\n",
    "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n",
    "                    In terms of the SVD from math class, this actually returns U * S\n",
    "    \"\"\"    \n",
    "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
    "    M_reduced = None\n",
    "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
    "    \n",
    "        # ------------------\n",
    "        # Write your implementation here.\n",
    "    M_r = sklearn.decomposition.TruncatedSVD(n_components=k,\n",
    "                                           algorithm='randomized', \n",
    "                                           n_iter=n_iters, \n",
    "                                           random_state=None, \n",
    "                                           tol=0.0)\n",
    "    M_reduced = M_r.fit_transform(M)\n",
    "        # ------------------\n",
    "\n",
    "    print(\"Done.\")\n",
    "    return M_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Truncated SVD over 70 words...\n",
      "Done.\n",
      "Running Truncated SVD over 30 words...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "kdim = 4\n",
    "train_sent_vector = reduce_to_k_dim(train_sent_vec, kdim)\n",
    "test_sent_vector = reduce_to_k_dim(test_sent_vec, kdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man prepares meal [ 1.3699642   0.05085485  0.914163   -0.1359012 ] [1.]\n"
     ]
    }
   ],
   "source": [
    "kn = 3\n",
    "print(training_data[kn],train_sent_vector[kn],train_labels[kn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QNN Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "*Quantum Classifer\n",
    "*Following code were adapted from paddle quantum, institute for quantum computing, Baidu Inc.\n",
    "*From:\n",
    "*https://qml.baidu.com/tutorials/machine-learning/quantum-classifier.html\n",
    "\"\"\"\n",
    "\n",
    "# Import numpy and paddle\n",
    "import numpy as np\n",
    "import paddle\n",
    "\n",
    "# To construct quantum circuit\n",
    "from paddle_quantum.circuit import UAnsatz\n",
    "# Some functions\n",
    "from numpy import pi as PI\n",
    "from paddle import matmul, transpose, reshape  # paddle matrix multiplication and transpose\n",
    "from paddle_quantum.utils import pauli_str_to_matrix, dagger  # N qubits Pauli matrix, complex conjugate\n",
    "\n",
    "# Plot figures, calculate the run time\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from paddle_quantum.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gate: rotate around Y-axis, Z-axis with angle theta\n",
    "def Ry(theta):\n",
    "    \"\"\"\n",
    "    :param theta: parameter\n",
    "    :return: Y rotation matrix\n",
    "    \"\"\"\n",
    "    return np.array([[np.cos(theta / 2), -np.sin(theta / 2)],\n",
    "                     [np.sin(theta / 2), np.cos(theta / 2)]])\n",
    "\n",
    "def Rz(theta):\n",
    "    \"\"\"\n",
    "    :param theta: parameter\n",
    "    :return: Z rotation matrix\n",
    "    \"\"\"\n",
    "    return np.array([[np.cos(theta / 2) - np.sin(theta / 2) * 1j, 0],\n",
    "                     [0, np.cos(theta / 2) + np.sin(theta / 2) * 1j]])\n",
    "\n",
    "# Classical -> Quantum Data Encoder\n",
    "def datapoints_transform_to_state(data, n_qubits):\n",
    "    \"\"\"\n",
    "    :param data: shape [-1, 2]\n",
    "    :param n_qubits: the number of qubits to which\n",
    "    the data transformed\n",
    "    :return: shape [-1, 1, 2 ^ n_qubits]\n",
    "        the first parameter -1 in this shape means can be arbitrary. In this tutorial, it equals to BATCH.\n",
    "    \"\"\"\n",
    "    dim1, dim2 = data.shape\n",
    "    res = []\n",
    "    for sam in range(dim1):\n",
    "        res_state = 1.\n",
    "        zero_state = np.array([[1, 0]])\n",
    "        # Angle Encoding\n",
    "        for i in range(n_qubits):\n",
    "            # For even number qubits, perform Rz(arccos(x0^2)) Ry(arcsin(x0))\n",
    "            if i % 2 == 0:\n",
    "                state_tmp=np.dot(zero_state, Ry(np.arcsin(data[sam][0])).T)\n",
    "                state_tmp=np.dot(state_tmp, Rz(np.arccos(data[sam][0] ** 2)).T)\n",
    "                res_state=np.kron(res_state, state_tmp)\n",
    "            # For odd number qubits, perform Rz(arccos(x1^2)) Ry(arcsin(x1))\n",
    "            elif i% 2 == 1:\n",
    "                state_tmp=np.dot(zero_state, Ry(np.arcsin(data[sam][1])).T)\n",
    "                state_tmp=np.dot(state_tmp, Rz(np.arccos(data[sam][1] ** 2)).T)\n",
    "                res_state=np.kron(res_state, state_tmp)\n",
    "        res.append(res_state)\n",
    "    res = np.array(res)\n",
    "\n",
    "    return res.astype(\"complex128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation of building a quantum neural network\n",
    "def cir_Classifier(theta, n, depth):\n",
    "    \"\"\"\n",
    "    :param theta: dim: [n, depth + 3], \"+3\" because we add an initial generalized rotation gate to each qubit\n",
    "    :param n: number of qubits\n",
    "    :param depth: circuit depth\n",
    "    :return: U_theta\n",
    "    \"\"\"\n",
    "    # Initialize the network\n",
    "    cir = UAnsatz(n)\n",
    "    \n",
    "    # Build a generalized rotation layer\n",
    "    for i in range(n):\n",
    "        cir.rz(theta[i][0], i)\n",
    "        cir.ry(theta[i][1], i)\n",
    "        cir.rz(theta[i][2], i)\n",
    "\n",
    "    # The default depth is depth = 1\n",
    "    # Build the entangleed layer and Ry rotation layer\n",
    "    for d in range(3, depth + 3):\n",
    "        # The entanglement layer\n",
    "        for i in range(n-1):\n",
    "            cir.cnot([i, i + 1])\n",
    "        cir.cnot([n-1, 0])\n",
    "        # Add Ry to each qubit\n",
    "        for i in range(n):\n",
    "            cir.ry(theta[i][d], i)\n",
    "\n",
    "    return cir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the computational graph\n",
    "class Opt_Classifier(paddle.nn.Layer):\n",
    "    \"\"\"\n",
    "    Construct the model net\n",
    "    \"\"\"\n",
    "    def __init__(self, n, depth, seed_paras=1, dtype='float64'):\n",
    "        # Initialization, use n, depth give the initial PQC\n",
    "        super(Opt_Classifier, self).__init__()\n",
    "        self.n = n\n",
    "        self.depth = depth\n",
    "        # Initialize the parameters theta with a uniform distribution of [0, 2*pi]\n",
    "        self.theta = self.create_parameter(\n",
    "            shape=[n, depth + 3],  # \"+3\" because we add an initial generalized rotation gate to each qubit\n",
    "            default_initializer=paddle.nn.initializer.Uniform(low=0.0, high=2*PI),\n",
    "            dtype=dtype,\n",
    "            is_bias=False)\n",
    "        # Initialize bias\n",
    "        self.bias = self.create_parameter(\n",
    "            shape=[1],\n",
    "            default_initializer=paddle.nn.initializer.Normal(std=0.01),\n",
    "            dtype=dtype,\n",
    "            is_bias=False)\n",
    "\n",
    "    # Define forward propagation mechanism, and then calculate loss function and cross-validation accuracy\n",
    "    def forward(self, state_in, label):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state_in: The input quantum state, shape [-1, 1, 2^n] -- in this tutorial: [BATCH, 1, 2^n]\n",
    "            label: label for the input state, shape [-1, 1]\n",
    "        Returns:\n",
    "            The loss:\n",
    "                L = 1/BATCH * ((<Z> + 1)/2 + bias - label)^2\n",
    "        \"\"\"\n",
    "        # Convert Numpy array to tensor\n",
    "        Ob = paddle.to_tensor(Observable(self.n))\n",
    "        label_pp = reshape(paddle.to_tensor(label), [-1, 1])\n",
    "\n",
    "        # Build the quantum circuit\n",
    "        cir = cir_Classifier(self.theta, n=self.n, depth=self.depth)\n",
    "        Utheta = cir.U\n",
    "\n",
    "        # Because Utheta is achieved by learning, we compute with row vectors to speed up without affecting the training effect\n",
    "        state_out = matmul(state_in, Utheta)  # shape:[-1, 1, 2 ** n], the first parameter is BATCH in this tutorial\n",
    "\n",
    "        # Measure the expectation value of Pauli Z operator <Z> -- shape [-1,1,1]\n",
    "        E_Z = matmul(matmul(state_out, Ob), transpose(paddle.conj(state_out), perm=[0, 2, 1]))\n",
    "\n",
    "        # Mapping <Z> to the estimated value of the label\n",
    "        state_predict = paddle.real(E_Z)[:, 0] * 0.5 + 0.5 + self.bias  # |y^{i,k} - \\tilde{y}^{i,k}|^2\n",
    "        loss = paddle.mean((state_predict - label_pp) ** 2)  # Get average for \"BATCH\" |y^{i,k} - \\tilde{y}^{i,k}|^2: L_i：shape:[1,1]\n",
    "\n",
    "        # Calculate the accuracy of cross-validation\n",
    "        is_correct = (paddle.abs(state_predict - label_pp) < 0.5).nonzero().shape[0]\n",
    "        acc = is_correct / label.shape[0]\n",
    "\n",
    "        return loss, acc, state_predict.numpy(), cir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Pauli Z operator that only acts on the first qubit\n",
    "# Act the identity matrix on rest of the qubits\n",
    "def Observable(n):\n",
    "    r\"\"\"\n",
    "    :param n: number of qubits\n",
    "    :return: local observable: Z \\otimes I \\otimes ...\\otimes I\n",
    "    \"\"\"\n",
    "    Ob = pauli_str_to_matrix([[1.0,'z0']], n)\n",
    "\n",
    "    return Ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simpler classifier\n",
    "def QClassifier2(quantum_train_x, train_y,quantum_test_x,test_y, N, DEPTH, EPOCH, LR, BATCH):\n",
    "    \"\"\"\n",
    "    Quantum Binary Classifier\n",
    "    Input：\n",
    "        quantum_train_x     # training x\n",
    "        train_y             # training y\n",
    "        quantum_test_x      # testing x\n",
    "        test_y              # testing y\n",
    "        N                   # Number of qubits required\n",
    "        DEPTH               # Circuit depth\n",
    "        EPOCH               # Number of training epochs\n",
    "        LR                  # Set the learning rate\n",
    "        BATCH               # Batch size during training\n",
    "    \"\"\"\n",
    "    Ntrain = len(quantum_train_x)\n",
    "    \n",
    "    paddle.seed(1)\n",
    "\n",
    "    net = Opt_Classifier(n=N, depth=DEPTH)\n",
    "\n",
    "    # Test accuracy list\n",
    "    summary_iter, summary_test_acc = [], []\n",
    "\n",
    "    # Adam can also be replaced by SGD or RMSprop\n",
    "    opt = paddle.optimizer.Adam(learning_rate=LR, parameters=net.parameters())\n",
    "\n",
    "    # Optimize\n",
    "    for ep in range(EPOCH):\n",
    "        for itr in range(Ntrain // BATCH):\n",
    "            # Import data\n",
    "            input_state = quantum_train_x[itr * BATCH:(itr + 1) * BATCH]  # paddle.tensor\n",
    "            input_state = reshape(input_state, [-1, 1, 2 ** N])\n",
    "            label = train_y[itr * BATCH:(itr + 1) * BATCH]\n",
    "            test_input_state = reshape(quantum_test_x, [-1, 1, 2 ** N])\n",
    "\n",
    "            loss, train_acc, state_predict_useless, cir = net(state_in=input_state, label=label)\n",
    "            # print(cir)\n",
    "            # print('---------------------------------------------------------------')\n",
    "            # if ep == 0 and itr == 0:\n",
    "            #     print(\"Circuit before training:\")\n",
    "            #     print(cir)\n",
    "            #     untrain_cir = cir\n",
    "            if itr % 20 == 0:\n",
    "                # get accuracy on test dataset (test_acc)\n",
    "                loss_useless, test_acc, state_predict_useless, t_cir = net(state_in=test_input_state, label=test_y)\n",
    "                print(\"epoch:\", ep, \"iter:\", itr,\n",
    "                      \"loss: %.4f\" % loss.numpy(),\n",
    "                      \"train acc: %.4f\" % train_acc,\n",
    "                      \"test acc: %.4f\" % test_acc)\n",
    "                summary_test_acc.append(test_acc)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.minimize(loss)\n",
    "            opt.clear_grad()\n",
    "\n",
    "    # print(\"Circuit before training:\")\n",
    "    # print(untrain_cir)\n",
    "\n",
    "    print(\"The trained circuit:\")\n",
    "    print(cir)\n",
    "\n",
    "    return summary_test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 iter: 0 loss: 0.2847 train acc: 0.3286 test acc: 0.5333\n",
      "epoch: 1 iter: 0 loss: 0.2805 train acc: 0.3857 test acc: 0.5333\n",
      "epoch: 2 iter: 0 loss: 0.2766 train acc: 0.4143 test acc: 0.6000\n",
      "epoch: 3 iter: 0 loss: 0.2731 train acc: 0.4000 test acc: 0.6000\n",
      "epoch: 4 iter: 0 loss: 0.2700 train acc: 0.4286 test acc: 0.5333\n",
      "epoch: 5 iter: 0 loss: 0.2672 train acc: 0.4857 test acc: 0.5000\n",
      "epoch: 6 iter: 0 loss: 0.2647 train acc: 0.5143 test acc: 0.5333\n",
      "epoch: 7 iter: 0 loss: 0.2626 train acc: 0.4857 test acc: 0.5667\n",
      "epoch: 8 iter: 0 loss: 0.2607 train acc: 0.5000 test acc: 0.5667\n",
      "epoch: 9 iter: 0 loss: 0.2590 train acc: 0.5571 test acc: 0.6000\n",
      "epoch: 10 iter: 0 loss: 0.2576 train acc: 0.5429 test acc: 0.5667\n",
      "epoch: 11 iter: 0 loss: 0.2562 train acc: 0.5714 test acc: 0.5667\n",
      "epoch: 12 iter: 0 loss: 0.2549 train acc: 0.5714 test acc: 0.5667\n",
      "epoch: 13 iter: 0 loss: 0.2537 train acc: 0.5714 test acc: 0.5667\n",
      "epoch: 14 iter: 0 loss: 0.2524 train acc: 0.5714 test acc: 0.5667\n",
      "epoch: 15 iter: 0 loss: 0.2511 train acc: 0.5714 test acc: 0.5667\n",
      "epoch: 16 iter: 0 loss: 0.2497 train acc: 0.5714 test acc: 0.5667\n",
      "epoch: 17 iter: 0 loss: 0.2483 train acc: 0.5714 test acc: 0.5667\n",
      "epoch: 18 iter: 0 loss: 0.2468 train acc: 0.5714 test acc: 0.5667\n",
      "epoch: 19 iter: 0 loss: 0.2453 train acc: 0.5714 test acc: 0.5667\n",
      "epoch: 20 iter: 0 loss: 0.2438 train acc: 0.5714 test acc: 0.5667\n",
      "epoch: 21 iter: 0 loss: 0.2422 train acc: 0.5714 test acc: 0.5667\n",
      "epoch: 22 iter: 0 loss: 0.2407 train acc: 0.5714 test acc: 0.6000\n",
      "epoch: 23 iter: 0 loss: 0.2392 train acc: 0.5857 test acc: 0.6000\n",
      "epoch: 24 iter: 0 loss: 0.2376 train acc: 0.6143 test acc: 0.6333\n",
      "epoch: 25 iter: 0 loss: 0.2361 train acc: 0.6286 test acc: 0.7000\n",
      "epoch: 26 iter: 0 loss: 0.2346 train acc: 0.6286 test acc: 0.7000\n",
      "epoch: 27 iter: 0 loss: 0.2330 train acc: 0.6286 test acc: 0.7333\n",
      "epoch: 28 iter: 0 loss: 0.2315 train acc: 0.6429 test acc: 0.7333\n",
      "epoch: 29 iter: 0 loss: 0.2300 train acc: 0.6429 test acc: 0.7000\n",
      "epoch: 30 iter: 0 loss: 0.2285 train acc: 0.6714 test acc: 0.7333\n",
      "epoch: 31 iter: 0 loss: 0.2269 train acc: 0.6857 test acc: 0.7667\n",
      "epoch: 32 iter: 0 loss: 0.2253 train acc: 0.6714 test acc: 0.7667\n",
      "epoch: 33 iter: 0 loss: 0.2237 train acc: 0.7000 test acc: 0.8000\n",
      "epoch: 34 iter: 0 loss: 0.2220 train acc: 0.7143 test acc: 0.8000\n",
      "epoch: 35 iter: 0 loss: 0.2203 train acc: 0.7143 test acc: 0.7667\n",
      "epoch: 36 iter: 0 loss: 0.2185 train acc: 0.7143 test acc: 0.7667\n",
      "epoch: 37 iter: 0 loss: 0.2166 train acc: 0.7143 test acc: 0.7667\n",
      "epoch: 38 iter: 0 loss: 0.2147 train acc: 0.7571 test acc: 0.7667\n",
      "epoch: 39 iter: 0 loss: 0.2127 train acc: 0.7571 test acc: 0.7667\n",
      "epoch: 40 iter: 0 loss: 0.2107 train acc: 0.7571 test acc: 0.7667\n",
      "epoch: 41 iter: 0 loss: 0.2086 train acc: 0.7571 test acc: 0.7667\n",
      "epoch: 42 iter: 0 loss: 0.2064 train acc: 0.7571 test acc: 0.7667\n",
      "epoch: 43 iter: 0 loss: 0.2042 train acc: 0.7714 test acc: 0.7667\n",
      "epoch: 44 iter: 0 loss: 0.2019 train acc: 0.7714 test acc: 0.7667\n",
      "epoch: 45 iter: 0 loss: 0.1995 train acc: 0.7714 test acc: 0.7667\n",
      "epoch: 46 iter: 0 loss: 0.1971 train acc: 0.7714 test acc: 0.7667\n",
      "epoch: 47 iter: 0 loss: 0.1946 train acc: 0.7714 test acc: 0.7667\n",
      "epoch: 48 iter: 0 loss: 0.1920 train acc: 0.7714 test acc: 0.7667\n",
      "epoch: 49 iter: 0 loss: 0.1894 train acc: 0.7714 test acc: 0.8000\n",
      "epoch: 50 iter: 0 loss: 0.1867 train acc: 0.7714 test acc: 0.8000\n",
      "epoch: 51 iter: 0 loss: 0.1840 train acc: 0.7857 test acc: 0.8000\n",
      "epoch: 52 iter: 0 loss: 0.1812 train acc: 0.8000 test acc: 0.8000\n",
      "epoch: 53 iter: 0 loss: 0.1783 train acc: 0.8143 test acc: 0.8000\n",
      "epoch: 54 iter: 0 loss: 0.1754 train acc: 0.8286 test acc: 0.8000\n",
      "epoch: 55 iter: 0 loss: 0.1725 train acc: 0.8286 test acc: 0.8000\n",
      "epoch: 56 iter: 0 loss: 0.1695 train acc: 0.8571 test acc: 0.8000\n",
      "epoch: 57 iter: 0 loss: 0.1666 train acc: 0.8571 test acc: 0.8000\n",
      "epoch: 58 iter: 0 loss: 0.1636 train acc: 0.8571 test acc: 0.8000\n",
      "epoch: 59 iter: 0 loss: 0.1606 train acc: 0.8571 test acc: 0.8000\n",
      "epoch: 60 iter: 0 loss: 0.1576 train acc: 0.8714 test acc: 0.8000\n",
      "epoch: 61 iter: 0 loss: 0.1546 train acc: 0.8714 test acc: 0.8000\n",
      "epoch: 62 iter: 0 loss: 0.1516 train acc: 0.8714 test acc: 0.8000\n",
      "epoch: 63 iter: 0 loss: 0.1487 train acc: 0.8714 test acc: 0.8000\n",
      "epoch: 64 iter: 0 loss: 0.1458 train acc: 0.8714 test acc: 0.8000\n",
      "epoch: 65 iter: 0 loss: 0.1429 train acc: 0.8714 test acc: 0.8000\n",
      "epoch: 66 iter: 0 loss: 0.1400 train acc: 0.8714 test acc: 0.8000\n",
      "epoch: 67 iter: 0 loss: 0.1372 train acc: 0.8857 test acc: 0.8000\n",
      "epoch: 68 iter: 0 loss: 0.1344 train acc: 0.8857 test acc: 0.8000\n",
      "epoch: 69 iter: 0 loss: 0.1317 train acc: 0.9143 test acc: 0.8000\n",
      "epoch: 70 iter: 0 loss: 0.1290 train acc: 0.9000 test acc: 0.8000\n",
      "epoch: 71 iter: 0 loss: 0.1264 train acc: 0.9143 test acc: 0.8000\n",
      "epoch: 72 iter: 0 loss: 0.1238 train acc: 0.9143 test acc: 0.8000\n",
      "epoch: 73 iter: 0 loss: 0.1213 train acc: 0.9143 test acc: 0.8000\n",
      "epoch: 74 iter: 0 loss: 0.1188 train acc: 0.9143 test acc: 0.8000\n",
      "epoch: 75 iter: 0 loss: 0.1164 train acc: 0.9143 test acc: 0.8000\n",
      "epoch: 76 iter: 0 loss: 0.1141 train acc: 0.9143 test acc: 0.8000\n",
      "epoch: 77 iter: 0 loss: 0.1119 train acc: 0.9143 test acc: 0.8333\n",
      "epoch: 78 iter: 0 loss: 0.1098 train acc: 0.9143 test acc: 0.8333\n",
      "epoch: 79 iter: 0 loss: 0.1077 train acc: 0.9143 test acc: 0.8333\n",
      "epoch: 80 iter: 0 loss: 0.1057 train acc: 0.9143 test acc: 0.8333\n",
      "epoch: 81 iter: 0 loss: 0.1039 train acc: 0.9143 test acc: 0.8333\n",
      "epoch: 82 iter: 0 loss: 0.1021 train acc: 0.9286 test acc: 0.8333\n",
      "epoch: 83 iter: 0 loss: 0.1004 train acc: 0.9286 test acc: 0.8333\n",
      "epoch: 84 iter: 0 loss: 0.0987 train acc: 0.9286 test acc: 0.8333\n",
      "epoch: 85 iter: 0 loss: 0.0972 train acc: 0.9286 test acc: 0.8667\n",
      "epoch: 86 iter: 0 loss: 0.0958 train acc: 0.9286 test acc: 0.8667\n",
      "epoch: 87 iter: 0 loss: 0.0944 train acc: 0.9286 test acc: 0.8667\n",
      "epoch: 88 iter: 0 loss: 0.0931 train acc: 0.9286 test acc: 0.8667\n",
      "epoch: 89 iter: 0 loss: 0.0919 train acc: 0.9286 test acc: 0.8667\n",
      "epoch: 90 iter: 0 loss: 0.0908 train acc: 0.9286 test acc: 0.8333\n",
      "epoch: 91 iter: 0 loss: 0.0898 train acc: 0.9286 test acc: 0.8333\n",
      "epoch: 92 iter: 0 loss: 0.0888 train acc: 0.9286 test acc: 0.8333\n",
      "epoch: 93 iter: 0 loss: 0.0879 train acc: 0.9286 test acc: 0.8333\n",
      "epoch: 94 iter: 0 loss: 0.0871 train acc: 0.9286 test acc: 0.8333\n",
      "epoch: 95 iter: 0 loss: 0.0863 train acc: 0.9286 test acc: 0.8667\n",
      "epoch: 96 iter: 0 loss: 0.0856 train acc: 0.9286 test acc: 0.8667\n",
      "epoch: 97 iter: 0 loss: 0.0849 train acc: 0.9286 test acc: 0.8667\n",
      "epoch: 98 iter: 0 loss: 0.0843 train acc: 0.9286 test acc: 0.8667\n",
      "epoch: 99 iter: 0 loss: 0.0837 train acc: 0.9286 test acc: 0.8667\n",
      "The trained circuit:\n",
      "--Rz(0.112)----Ry(5.936)----Rz(4.630)----*--------------x----Ry(1.457)----*--------------x----Ry(2.075)----*--------------x----Ry(6.088)--\n",
      "                                         |              |                 |              |                 |              |               \n",
      "--Rz(3.918)----Ry(4.081)----Rz(3.376)----x----*---------|----Ry(0.411)----x----*---------|----Ry(3.184)----x----*---------|----Ry(4.321)--\n",
      "                                              |         |                      |         |                      |         |               \n",
      "--Rz(2.177)----Ry(3.950)----Rz(3.441)---------x----*----|----Ry(1.511)---------x----*----|----Ry(3.240)---------x----*----|----Ry(5.809)--\n",
      "                                                   |    |                           |    |                           |    |               \n",
      "--Rz(4.937)----Ry(4.822)----Rz(2.240)--------------x----*----Ry(4.651)--------------x----*----Ry(1.786)--------------x----*----Ry(3.387)--\n",
      "                                                                                                                                          \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "test_rate = 0.2\n",
    "num_qubit = 4\n",
    "dimension = 4\n",
    "# acquire Iris data as quantum states\n",
    "# iris =Iris (encoding='angle_encoding', num_qubits=num_qubit, test_rate=test_rate,classes=[0, 1], return_state=True)\n",
    "\n",
    "# quantum_train_x, train_y = iris.train_x, iris.train_y\n",
    "# quantum_test_x, test_y = iris.test_x, iris.test_y\n",
    "# testing_data_num = len(test_y)\n",
    "# training_data_num = len(train_y)\n",
    "quantum_train_x= SimpleDataset(dimension).encode(train_sent_vector, 'angle_encoding', num_qubit)\n",
    "quantum_test_x= SimpleDataset(dimension).encode(test_sent_vector, 'angle_encoding', num_qubit)\n",
    "quantum_train_x = paddle.to_tensor(quantum_train_x)\n",
    "quantum_test_x = paddle.to_tensor(quantum_test_x)\n",
    "    \n",
    "\n",
    "acc = QClassifier2(\n",
    "        quantum_train_x, \n",
    "        train_labels,         \n",
    "        quantum_test_x,  \n",
    "        test_labels,          \n",
    "        N = num_qubit,   \n",
    "        DEPTH = 3,       \n",
    "        EPOCH = 100,       \n",
    "        LR = 0.01,        \n",
    "        BATCH = 70,       \n",
    "      )\n",
    "plt.plot(acc)\n",
    "plt.title(\"Classify text 0&1 using angle encoding\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Testing accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(gamma='auto')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = svm.SVC(gamma='auto')\n",
    "SVM.fit(train_sent_vector,train_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  100.0\n"
     ]
    }
   ],
   "source": [
    "predictions_SVM = SVM.predict(test_sent_vector)\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, test_l)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_df ={'Prediction':predictions_SVM.tolist(),'Truth':test_l}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Prediction  Truth\n",
       "0            1      1\n",
       "1            1      1\n",
       "2            0      0\n",
       "3            1      1\n",
       "4            1      1\n",
       "5            0      0\n",
       "6            0      0\n",
       "7            0      0\n",
       "8            1      1\n",
       "9            1      1\n",
       "10           1      1\n",
       "11           0      0\n",
       "12           0      0\n",
       "13           1      1\n",
       "14           1      1\n",
       "15           0      0\n",
       "16           0      0\n",
       "17           0      0\n",
       "18           0      0\n",
       "19           0      0\n",
       "20           0      0\n",
       "21           1      1\n",
       "22           1      1\n",
       "23           1      1\n",
       "24           0      0\n",
       "25           1      1\n",
       "26           0      0\n",
       "27           1      1\n",
       "28           0      0\n",
       "29           1      1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(svm_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextCnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=100, output_sequence_length=200)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(training_data).batch(128)\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=['IT','Food']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 17 words (0 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 300\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= 2:\n",
    "        embedding_vector = wv[word]\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            # This includes the representation for \"padding\" and \"OOV\"\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "    \n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 300)         6300      \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 128)         192128    \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, None, 128)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, None, 128)        0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 379,294\n",
      "Trainable params: 372,994\n",
      "Non-trainable params: 6,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "# fz = 4\n",
    "# ker =1\n",
    "\n",
    "fz = 128\n",
    "ker =5\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = layers.Conv1D(fz, ker, activation=\"relu\")(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(fz, ker, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(fz, ker, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(fz, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "model = keras.Model(int_sequences_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorizer(np.array([[s] for s in training_data])).numpy()\n",
    "x_val = vectorizer(np.array([[s] for s in testing_data])).numpy()\n",
    "\n",
    "y_train = np.array(train_l)\n",
    "y_val = np.array(test_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 18s 18s/step - loss: 0.6946 - acc: 0.4857 - val_loss: 0.6658 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6570 - acc: 0.6286 - val_loss: 0.5477 - val_acc: 0.8667\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5212 - acc: 0.9000 - val_loss: 0.3289 - val_acc: 1.0000\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2906 - acc: 1.0000 - val_loss: 0.1389 - val_acc: 1.0000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1320 - acc: 1.0000 - val_loss: 0.0525 - val_acc: 1.0000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0603 - acc: 1.0000 - val_loss: 0.0348 - val_acc: 1.0000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0314 - acc: 1.0000 - val_loss: 0.0177 - val_acc: 1.0000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0173 - acc: 1.0000 - val_loss: 0.0144 - val_acc: 1.0000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0143 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7.6912e-04 - acc: 1.0000 - val_loss: 9.4560e-04 - val_acc: 1.0000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 7.5473e-04 - val_acc: 1.0000\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 9.7710e-04 - acc: 1.0000 - val_loss: 6.7784e-04 - val_acc: 1.0000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 5.6649e-04 - val_acc: 1.0000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.9251e-04 - acc: 1.0000 - val_loss: 4.9413e-04 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f80f9848e0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=70, epochs=20, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = vectorizer(string_input)\n",
    "preds = model(x)\n",
    "end_to_end_model = keras.Model(string_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "for sent in dev_data:\n",
    "    probabilities = end_to_end_model.predict([[sent]])\n",
    "    pred.append(class_names[np.argmax(probabilities[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 it . 1 food\n",
    "true = []\n",
    "for ele in dev_l:\n",
    "    if ele == 1:\n",
    "        true.append('Food')\n",
    "    else:\n",
    "        true.append('IT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IT</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Food</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Food</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Food</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IT</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IT</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Food</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Food</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IT</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IT</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>IT</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>IT</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Food</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>IT</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Food</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>IT</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Food</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>IT</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>IT</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Food</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>IT</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>IT</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Food</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>IT</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>IT</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>IT</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>IT</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>IT</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>IT</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Food</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prediction Truth\n",
       "0          IT    IT\n",
       "1        Food  Food\n",
       "2        Food  Food\n",
       "3        Food  Food\n",
       "4          IT    IT\n",
       "5          IT    IT\n",
       "6        Food  Food\n",
       "7        Food  Food\n",
       "8          IT    IT\n",
       "9          IT    IT\n",
       "10         IT    IT\n",
       "11         IT    IT\n",
       "12       Food  Food\n",
       "13         IT    IT\n",
       "14       Food  Food\n",
       "15         IT    IT\n",
       "16       Food  Food\n",
       "17         IT    IT\n",
       "18         IT    IT\n",
       "19       Food  Food\n",
       "20         IT    IT\n",
       "21         IT    IT\n",
       "22       Food  Food\n",
       "23         IT    IT\n",
       "24         IT    IT\n",
       "25         IT    IT\n",
       "26         IT    IT\n",
       "27         IT    IT\n",
       "28         IT    IT\n",
       "29       Food  Food"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_df ={'Prediction':pred,'Truth':true}\n",
    "cnn_df=pd.DataFrame(cnn_df)\n",
    "cnn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01e8a6da9a49d9104296038562020976433a3d921661fdda7bcdf390e6a6d8cf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('qc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
